{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtgVTGuJGjUT"
      },
      "source": [
        "<h1>NLP with ML - Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "wfof4NDtkuEk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import*\n",
        "import pandas as pd\n",
        "import re\n",
        "from functools import reduce \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "dataFrame=pd.read_csv('/content/data2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "DA_huKWNmJrY"
      },
      "outputs": [],
      "source": [
        "def add1_smoothing(df, words):\n",
        "  cWith = 0\n",
        "  while cWith < len(words):\n",
        "    if words[cWith] == 'ile':\n",
        "        del words[cWith]\n",
        "    cWith = cWith + 1\n",
        "  \n",
        "  dataDuygu = []\n",
        "  dataYorum = []\n",
        "  dataDuyguSentence = []\n",
        "  for i in range(len(df)):\n",
        "    dataDuyguSentence.append(df['Duygu'][i])\n",
        "    for j in df['Yorum'][i].split():\n",
        "      dataYorum.append(j)\n",
        "      dataDuygu.append(df['Duygu'][i])\n",
        "\n",
        "  tokenizer = Tokenizer(num_words = 1000)\n",
        "  tokenizer.fit_on_texts(df['Yorum'])\n",
        "  word_index = tokenizer.word_index\n",
        "  vocab = len(tokenizer.word_counts)\n",
        "  \n",
        "  numOfPositive = dataDuygu.count(1)\n",
        "  numOfNegative = dataDuygu.count(0)\n",
        "  \n",
        "  pPositive={}\n",
        "  pNegative={}\n",
        "  keyDictPositive = {}\n",
        "  keyDictNegative = {}\n",
        "  for i in words:\n",
        "    keyDictPositive.update({i:0})\n",
        "    keyDictNegative.update({i:0})\n",
        "    pPositive.update({i:1})\n",
        "    pNegative.update({i:1})\n",
        "\n",
        "  counter=0\n",
        "  for i in dataYorum:\n",
        "    for j in words:\n",
        "      if (j in i) and dataDuygu[counter]==1:\n",
        "        keyDictPositive.update({j:keyDictPositive[j]+1})\n",
        "      if (j in i) and dataDuygu[counter]==0:\n",
        "        keyDictNegative.update({j:keyDictNegative[j]+1})\n",
        "    counter+=1\n",
        "  \n",
        "  for i in words:\n",
        "    pPositive.update({i:(keyDictPositive[i]+1)/(vocab+numOfPositive)})\n",
        "    pNegative.update({i:(keyDictNegative[i]+1)/(vocab+numOfNegative)})\n",
        "\n",
        "  pNumOfPositive = dataDuyguSentence.count(1)\n",
        "  pNumOfNegative = dataDuyguSentence.count(0)\n",
        "  numOfSentenceTotal = pNumOfPositive + pNumOfNegative\n",
        "  pTotalPositive = pNumOfPositive / numOfSentenceTotal\n",
        "  pTotalNegative = pNumOfNegative / numOfSentenceTotal\n",
        "\n",
        "  for i in words:\n",
        "    pTotalPositive *= pPositive[i]\n",
        "    pTotalNegative *= pNegative[i]\n",
        "  \n",
        "  if pTotalPositive > pTotalNegative:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "xi7YB-rm3nhi"
      },
      "outputs": [],
      "source": [
        "def negation(df):\n",
        "  data=df['Yorum']\n",
        "  dataDuygu=df['Duygu']\n",
        "  pattern = r'\\b(?:değil|yok)'\n",
        "  dataNegation = []\n",
        "  for c in (range(len(data))):\n",
        "    neg = []\n",
        "    endPoints = []\n",
        "    if re.search(r'[?.,*!:;]',data[c])==None:\n",
        "      endPoints.append(len(data[c]))\n",
        "    for match in (re.finditer('[?.,*!:;]', data[c])):\n",
        "      endPoints.append(match.end())\n",
        "    for i in range(len(endPoints)):\n",
        "      if i==0:\n",
        "        if len(re.findall(pattern,data[c][0:endPoints[i]]))!=0:\n",
        "          sentenceArray = data[c][0:endPoints[i]].split()\n",
        "          for j in range(len(sentenceArray)):\n",
        "            if re.search(pattern, sentenceArray[j])==None:\n",
        "              sentenceArray[j]='değil_'+sentenceArray[j]\n",
        "            else:\n",
        "              sentenceArray[j]=sentenceArray[j]\n",
        "        else:\n",
        "          sentenceArray = data[c][0:endPoints[i]].split()\n",
        "        neg.append(\" \".join(sentenceArray))\n",
        "      elif i!=len(endPoints):\n",
        "        if len(re.findall(pattern,data[c][endPoints[i-1]:endPoints[i]]))!=0:\n",
        "          sentenceArray = data[c][endPoints[i-1]:endPoints[i]].split()\n",
        "          for j in range(len(sentenceArray)):\n",
        "            if re.search(pattern, sentenceArray[j])==None:\n",
        "              sentenceArray[j]='değil_'+sentenceArray[j]\n",
        "            else:\n",
        "              sentenceArray[j]=sentenceArray[j]\n",
        "        else:\n",
        "          sentenceArray = data[c][endPoints[i-1]:endPoints[i]].split()\n",
        "        neg.append(\" \".join(sentenceArray))\n",
        "        if len(data[c])!=endPoints[-1]:\n",
        "          if len(re.findall(pattern,data[c][endPoints[-1]:len(data[c])]))!=0:\n",
        "            sentenceArray = data[c][endPoints[-1]:len(data[c])].split()\n",
        "            for j in range(len(sentenceArray)):\n",
        "              if re.search(pattern, sentenceArray[j])==None:\n",
        "                sentenceArray[j]='değil_'+sentenceArray[j]\n",
        "              else:\n",
        "                sentenceArray[j]=sentenceArray[j]\n",
        "          else:\n",
        "            sentenceArray = data[c][endPoints[-1]:len(data[c])].split()\n",
        "          neg.append(\" \".join(sentenceArray))\n",
        "    dataNegation.append(\" \".join(neg))\n",
        "  frame=[]\n",
        "  for i in range(len(dataNegation)):\n",
        "    frame.append([dataNegation[i],dataDuygu[i]])\n",
        "  dfNegation=pd.DataFrame(frame,columns = ['Yorum', 'Duygu'])\n",
        "  return dfNegation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "7i1tWwRBKX36"
      },
      "outputs": [],
      "source": [
        "def lexicon(df):\n",
        "  dataYorum = df[\"Yorum\"]\n",
        "  dataDuygu = df['Duygu']\n",
        "  dataPositive = []\n",
        "  dataNegative = []\n",
        "\n",
        "  for i in range(len(dataYorum)):\n",
        "    if dataDuygu[i] == 1:\n",
        "      dataPositive.append(dataYorum[i])\n",
        "    else:\n",
        "      dataNegative.append(dataYorum[i])\n",
        "  tokenizerPositive = Tokenizer(num_words = 1000)\n",
        "  tokenizerPositive.fit_on_texts(dataPositive)\n",
        "  word_index_positive = tokenizerPositive.word_counts\n",
        "  tokenizerNegative = Tokenizer(num_words = 1000)\n",
        "  tokenizerNegative.fit_on_texts(dataNegative)\n",
        "  word_index_negative = tokenizerNegative.word_counts\n",
        "  vocab = len(tokenizerPositive.word_counts)+len(tokenizerNegative.word_counts) \n",
        "  allPositiveCount=0\n",
        "  allNegativeCount=0\n",
        "  \n",
        "  for key,value in word_index_positive.items():\n",
        "    allPositiveCount+=value\n",
        "  for key,value in word_index_negative.items():\n",
        "    allNegativeCount+=value\n",
        "  \n",
        "  lexiconDuygu=[]\n",
        "  lexiconYorumScore=[]\n",
        "  lexiconYorum=[]\n",
        "  lexiconPositive={}\n",
        "  lexiconNegative={}\n",
        "  for key,value in word_index_positive.items():\n",
        "    lexiconYorum.append(key)\n",
        "    lexiconYorumScore.append((value+1)/(allPositiveCount+vocab))\n",
        "    lexiconDuygu.append(1)\n",
        "    lexiconPositive.update({key:(value+1)/(allPositiveCount+vocab)})\n",
        "  for key,value in word_index_negative.items():\n",
        "    lexiconYorum.append(key)\n",
        "    lexiconYorumScore.append((value+1)/(allNegativeCount+vocab))   \n",
        "    lexiconDuygu.append(-1)\n",
        "    lexiconNegative.update({key:(value+1)/(allNegativeCount+vocab)})\n",
        "\n",
        "  frame=[]\n",
        "  for i in range(len(lexiconYorum)):\n",
        "    frame.append([lexiconYorum[i],lexiconYorumScore[i],lexiconDuygu[i]])\n",
        "  lexicon=pd.DataFrame(frame,columns = ['Yorum',\"Score\", 'Duygu'])\n",
        "  return lexicon,lexiconPositive,lexiconNegative\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "vJ2FRhVJwDSk"
      },
      "outputs": [],
      "source": [
        "dataframe, y_Test = train_test_split(dataFrame,test_size=0.05,random_state=0)\n",
        "y_pred = []\n",
        "y_pred_negation = [] \n",
        "y_true = []\n",
        "for i in y_Test['Duygu']:\n",
        "  y_true.append(i)\n",
        "for i in y_Test['Yorum']:\n",
        "  y_pred.append(add1_smoothing(dataFrame, i.split()))\n",
        "  y_pred_negation.append(add1_smoothing(negation(dataFrame),i.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "0HhJ1joqY9Uc"
      },
      "outputs": [],
      "source": [
        "lex,lex_test = train_test_split(lexicon(dataFrame)[0],test_size=0.001,random_state=5)\n",
        "lexP=lexicon(dataFrame)[1]\n",
        "lexN=lexicon(dataFrame)[2]\n",
        "# Positive or negative labels have been set manually.\n",
        "testArray=['çok','iyi','orta','fiyat','tavsiye','telefon','kadar','kart','kamera','cihaz']\n",
        "lex_pred = []\n",
        "lex_true = [1,1,0,0,1,0,0,1,1,0]\n",
        "\n",
        "for i in testArray:\n",
        "  if lexP[i]>=lexN[i]:\n",
        "    lex_pred.append(1)\n",
        "  else:\n",
        "    lex_pred.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PKUnAi_JnuB",
        "outputId": "04a7e785-3a2a-4e23-f08f-89a67895dc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Add1 Smoothing Scores\n",
            "0.9591836734693877\n",
            "0.9591836734693877\n",
            "0.9591836734693877\n",
            "0.9591836734693877\n",
            "Add1 Smoothing with Negation Scores\n",
            "0.9591836734693877\n",
            "0.9591836734693877\n",
            "0.9591836734693877\n",
            "0.9591836734693877\n",
            "Lexicon Scores\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "print('Add1 Smoothing Scores')\n",
        "print(accuracy_score(y_true, y_pred))\n",
        "print(precision_score(y_true, y_pred, average='micro'))\n",
        "print(recall_score(y_true, y_pred, average='micro'))\n",
        "print(f1_score(y_true, y_pred, average='micro'))\n",
        "print('Add1 Smoothing with Negation Scores')\n",
        "print(accuracy_score(y_true, y_pred_negation))\n",
        "print(precision_score(y_true, y_pred_negation, average='micro'))\n",
        "print(recall_score(y_true, y_pred_negation, average='micro'))\n",
        "print(f1_score(y_true, y_pred_negation, average='micro'))\n",
        "print('Lexicon Scores')\n",
        "print(accuracy_score(lex_true, lex_pred))\n",
        "print(precision_score(lex_true, lex_pred, average='micro'))\n",
        "print(recall_score(lex_true, lex_pred, average='micro'))\n",
        "print(f1_score(lex_true, lex_pred, average='micro'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
